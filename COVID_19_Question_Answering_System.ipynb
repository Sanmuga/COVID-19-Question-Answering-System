{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanmuga/COVID-19-Question-Answering-System/blob/main/COVID_19_Question_Answering_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFnyvtcfUk0x",
        "outputId": "6b85c775-ef59-4a7f-926e-55f2272bd4d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')\n",
        "import re\n",
        "import string\n",
        "import gensim\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy import spatial\n",
        "from nltk import pos_tag,word_tokenize,ne_chunk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pandas import DataFrame\n",
        "\n",
        "from nltk.corpus import wordnet,stopwords\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "kXDvkUG1Ux3g",
        "outputId": "c6c7227c-c596-4cab-e80d-2ce3bdfe2aa9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b24f6719-b80c-422b-8bd3-5a7967bc1f7e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b24f6719-b80c-422b-8bd3-5a7967bc1f7e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving wikipedia.txt to wikipedia (1).txt\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded=files.upload()\n",
        "sample = open(\"wikipedia.txt\", \"r\")\n",
        "\n",
        "s = sample.read()\n",
        "s=s.replace(\"COVID 19\",\"coronavirus\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYxhviR4U3MD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def stem_sentence(sentence):\n",
        "  words=word_tokenize(sentence)\n",
        "  #lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "  stemmer = SnowballStemmer(\"english\")\n",
        "  new_words=[]\n",
        "  for i in words:\n",
        "    new_words.append(stemmer.stem(i))\n",
        "    new_words.append(\" \")\n",
        "  return \"\".join(new_words)\n",
        "\n",
        "\n",
        "\n",
        "def clean_sentence(sentence, stopwords=True):\n",
        "\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
        "\n",
        "\n",
        "\n",
        "    if stopwords:\n",
        "         sentence = remove_stopwords(sentence)\n",
        "\n",
        "\n",
        "\n",
        "    return sentence\n",
        "def get_cleaned_sentences(sents,stopwords=True):\n",
        "\n",
        "    cleaned_sentences=[]\n",
        "\n",
        "    for i in sents:\n",
        "\n",
        "        cleaning=clean_sentence(i,stopwords)\n",
        "        cleaned=stem_sentence(cleaning)\n",
        "        cleaned_sentences.append(cleaned)\n",
        "    return cleaned_sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jpLxPbzWMbv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_document_term_matrix(sen,vectorizer):\n",
        "  doc_term_matrix=vectorizer.fit_transform(sen)\n",
        "  return DataFrame(doc_term_matrix.toarray(), columns=vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIvi-NpNWcli"
      },
      "outputs": [],
      "source": [
        "def calculate_cosine_similarity(df_list,sentences,question):\n",
        "  a=[]\n",
        "  for i in range(len(df_list)-1):\n",
        "    sim=1 - spatial.distance.cosine(df_list[i], question)\n",
        "    t=(sim,sentences[i])\n",
        "    a.append(t)\n",
        "  a.sort(reverse=True)\n",
        "  n=[]\n",
        "  for i in range(3):\n",
        "    n.append(a[i][1])\n",
        "    #print(\"*\",n[i])\n",
        "\n",
        "\n",
        "  return n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlUttTFcWkB7"
      },
      "outputs": [],
      "source": [
        "def questiontype( question):\n",
        "        questiontags = ['WP','WDT','WP$','WRB']\n",
        "        question_POS = pos_tag(word_tokenize(question.lower()))\n",
        "\n",
        "        question_Tags=[]\n",
        "        for token in question_POS:\n",
        "            if token[1] in questiontags:\n",
        "              question_Tags.append(token)\n",
        "\n",
        "\n",
        "        if len(question_Tags)==1 and question_Tags[0][0]!= 'what' :\n",
        "          return True\n",
        "        else:\n",
        "          return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3GFT1kyWoUc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from nltk.util import ngrams\n",
        "def n_gram_similarity(question,n):\n",
        "  q=list(ngrams(word_tokenize(question.lower()),1))\n",
        "  a=0\n",
        "  b=0\n",
        "  c=0\n",
        "  t=[]\n",
        "  for i in q:\n",
        "    if i in list(ngrams(word_tokenize(n[0].lower()),1)):\n",
        "      a=a+1\n",
        "  for i in q:\n",
        "    if i in list(ngrams(word_tokenize(n[1].lower()),1)):\n",
        "      b=b+1\n",
        "  for i in q:\n",
        "    if i in list(ngrams(word_tokenize(n[2].lower()),1)):\n",
        "      c=c+1\n",
        "  d=max(a,b,c)\n",
        "  if a == d:\n",
        "    t.append(n[0])\n",
        "  if b == d:\n",
        "    t.append(n[1])\n",
        "  if c ==d:\n",
        "    t.append(n[2])\n",
        "  print()\n",
        "  #print(\"Selected Sentence:\",t[0])\n",
        "  return t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6YQUrWMWpUT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def answertype(question):\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "  if (questiontype(question)):\n",
        "    t='DESCRIPTIVE'\n",
        "    flag=0\n",
        "    word=word_tokenize(question.lower())\n",
        "\n",
        "    if 'who' in word:\n",
        "      t='PERSON'\n",
        "    elif 'where' in word:\n",
        "      t='GPE'\n",
        "    elif 'how' in word and 'many' in word and  'age' in word or 'duration' in word or 'long' in word or 'days'in word or 'years' in word or'months' in word:\n",
        "      t='DATE'\n",
        "    elif 'how' in word and 'many' in word :\n",
        "       t = 'CARDINAL'\n",
        "    elif 'when' in word  or 'age' in word or 'period' in word or 'duration' in word  or 'old' in word or 'long' in word:\n",
        "      t='DATE'\n",
        "    elif 'how' in word  and 'long' in word or 'often' or 'age' in word or 'years' in word:\n",
        "      t='DATE'\n",
        "    elif 'what' in word and 'time' in word or 'duration' in word or 'period' in  'word'  :\n",
        "      t='DATE'\n",
        "    i=len(df_list)-1\n",
        "    n=calculate_cosine_similarity(df_list, sentences,df_list[i])\n",
        "    n=n_gram_similarity(question,n)\n",
        "    #print(\"Most relevant sentence\", n[0])\n",
        "    #print(\"ANSWER TYPE:\",t)\n",
        "    key = n[0]\n",
        "    spdoc = nlp(key)\n",
        "    entity_type=[]\n",
        "    for ent in spdoc.ents:\n",
        "       if ent.label_ == t:\n",
        "          entity_type.append(ent.text)\n",
        "    if len(entity_type) == 1:\n",
        "      #print(\"ANSWER TYPE:\", t)\n",
        "      print(\"ANSWER:\", entity_type[0])\n",
        "    if len(entity_type) == 0:\n",
        "      #print(\"ANSWER TYPE:\", t)\n",
        "      print(n[0])\n",
        "    if len(entity_type) > 1:\n",
        "      #print(\"Answer Type:\",t)\n",
        "      key_question = question\n",
        "      q=[]\n",
        "      spdoc = nlp(key_question)\n",
        "      for ent in spdoc:\n",
        "        if ent.pos_ == 'NOUN' or ent.pos_ =='ADJ' :\n",
        "          q.append(ent.text)\n",
        "\n",
        "      key_answer = n[0]\n",
        "      a = []\n",
        "      spd = nlp(key)\n",
        "      for ent in spd:\n",
        "        if ent.pos_ == 'NOUN'or ent.pos_ =='ADJ' :\n",
        "          a.append(ent.text)\n",
        "  #s=[sentence.index(i) for i in t]\n",
        "      s=[]\n",
        "      w=[]\n",
        "      for i in entity_type:\n",
        "       s.append(n[0].index(i))\n",
        "      for i in range(len(s)):\n",
        "        w.append(0)\n",
        "\n",
        "\n",
        "      for i in q:\n",
        "        try:\n",
        "           factor= n[0].index(i)\n",
        "           for j in range(len(s)):\n",
        "              w[j]=w[j]+(abs(s[j]-factor))\n",
        "        except:\n",
        "           continue\n",
        "      m=min(w)\n",
        "      u=[]\n",
        "      for i in range(len(s)):\n",
        "        if w[i] == m:\n",
        "           #print(entity_type[i])\n",
        "           u.append(entity_type[i])\n",
        "      print(\"ANSWER:\",u[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  else:\n",
        "    t='DESCRIPTIVE'\n",
        "    #print(\"ANSWER TYPE:\",t)\n",
        "    i=len(df_list)-1\n",
        "    n=calculate_cosine_similarity(df_list, sentences,df_list[i])\n",
        "    #n = n_gram_similarity(question, n)\n",
        "    for j in n:\n",
        "         print(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab0o8XLoWwQR",
        "outputId": "e2199e94-3324-4b74-947a-ab93355a0738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUESTION: How many cases have been reported and how many deaths have occured in various countries?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Lymphocytic infiltrates have also been reported at autopsy.\n",
            "\n",
            "\n",
            "ANSWER: six\n",
            "\n",
            "QUESTION: What are the common symptoms and how long should I wash my hands?\n",
            "As is common with infections, there is a delay between the moment a person is first infected and the time he or she develops symptoms.\n",
            "Common symptoms include fever, cough, fatigue, shortness of breath, and loss of smell and taste.\n",
            "Common symptoms include headache, loss of smell and taste, nasal congestion and rhinorrhea, cough, muscle pain, sore throat, fever and breathing difficulties.People with the same infection may have different symptoms, and their symptoms may change over time.\n",
            "\n",
            "\n",
            "The CDC also recommends that individuals wash hands often with soap and water for at least 20 seconds, especially after going to the toilet or when hands are visibly dirty, before eating and after blowing one's nose, coughing or sneezing.\n",
            "\n",
            "QUESTION: How many people have recovered till now and how many cases have been reported?\n",
            "\n",
            "ANSWER: More than 4.33 million\n",
            "\n",
            "\n",
            "Lymphocytic infiltrates have also been reported at autopsy.\n",
            "\n",
            "QUESTION: How far apart should people stay and how long should I wash my hands ?\n",
            "\n",
            "Distancing guidelines also include that people stay at least 6 feet (1.8 m) apart.\n",
            "\n",
            "\n",
            "The CDC also recommends that individuals wash hands often with soap and water for at least 20 seconds, especially after going to the toilet or when hands are visibly dirty, before eating and after blowing one's nose, coughing or sneezing.\n",
            "\n",
            "QUESTION: When was the first case discovered and where was the virus first identified?\n",
            "\n",
            "ANSWER: 55 days\n",
            "\n",
            "\n",
            "ANSWER: Wuhan\n",
            "\n",
            "QUESTION: When was the first case discoverd  ?\n",
            "\n",
            "ANSWER: 55 days\n",
            "\n",
            "QUESTION: Where was the virus first identified?\n",
            "\n",
            "ANSWER: Wuhan\n",
            "\n",
            "QUESTION: When was the virus first identified?\n",
            "\n",
            "ANSWER: December 2019\n",
            "\n",
            "QUESTION: When did the first case occur in Delhi?\n",
            "\n",
            "ANSWER: 2nd March\n",
            "\n",
            "QUESTION: How many days is the incubation period of the virus?\n",
            "\n",
            "ANSWER: five or six days\n",
            "\n",
            "QUESTION: When was COVID-19 declared a health emergency?\n",
            "\n",
            "ANSWER: 30 January\n",
            "\n",
            "QUESTION: When was COVID-19 declared a pandemic?\n",
            "\n",
            "ANSWER: 11 March 2020\n",
            "\n",
            "QUESTION: How apart should people stay?\n",
            "\n",
            "Distancing guidelines also include that people stay at least 6 feet (1.8 m) apart.\n",
            "\n",
            "QUESTION: How many people have recovered till now?\n",
            "\n",
            "ANSWER: More than 4.33 million\n",
            "\n",
            "QUESTION: How many cases have been reported?\n",
            "\n",
            "Lymphocytic infiltrates have also been reported at autopsy.\n",
            "\n",
            "QUESTION: How many deaths have occured in various countries?\n",
            "\n",
            "ANSWER: six\n",
            "\n",
            "QUESTION: In how many World Health Organization zones local transmission has occuerd?\n",
            "\n",
            "ANSWER: six\n",
            "\n",
            "QUESTION: Cases have been reported in how many countries?\n",
            "\n",
            "ANSWER: 188\n",
            "\n",
            "QUESTION: How many countries have reported cases?\n",
            "\n",
            "ANSWER: 188\n",
            "\n",
            "QUESTION: How long should I wash my hands?\n",
            "\n",
            "The CDC also recommends that individuals wash hands often with soap and water for at least 20 seconds, especially after going to the toilet or when hands are visibly dirty, before eating and after blowing one's nose, coughing or sneezing.\n",
            "\n",
            "QUESTION: What are the common symptoms of COVID-19?\n",
            "Symptoms of COVID-19 are variable, ranging from mild symptoms to severe illness.\n",
            "As is common with infections, there is a delay between the moment a person is first infected and the time he or she develops symptoms.\n",
            "Fever is the most common symptom of COVID-19, but is highly variable in severity and presentation, with some older, immunocompromised, or critically ill people not having fever at all.\n",
            "\n",
            "QUESTION: What type of masks should I wear?\n",
            "Social distancing and the wearing of cloth face masks, surgical masks, respirators, or other face coverings are controls for droplet transmission.\n",
            "Health officials also stated that medical-grade face masks, such as N95 masks, should only be used by healthcare workers, first responders, and those who directly care for infected individuals.\n",
            "Several countries have recommended that healthy individuals wear face masks or cloth face coverings (like scarves or bandanas) at least in certain public settings, including China, Hong Kong, Spain, Italy, Russia, and the United States.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#from textblob import TextBlob\n",
        "\n",
        "question=[\"How many cases have been reported and how many deaths have occured in various countries?\",\"What are the common symptoms and how long should I wash my hands?\",\"How many people have recovered till now and how many cases have been reported?\",\"How far apart should people stay and how long should I wash my hands ?\",\"When was the first case discovered and where was the virus first identified?\",\"When was the first case discoverd  ?\",\"Where was the virus first identified?\",\"When was the virus first identified?\",\"When did the first case occur in Delhi?\",\"How many days is the incubation period of the virus?\",\"When was COVID-19 declared a health emergency?\",\"When was COVID-19 declared a pandemic?\",\"How apart should people stay?\",\"How many people have recovered till now?\",\"How many cases have been reported?\",\"How many deaths have occured in various countries?\",\"In how many World Health Organization zones local transmission has occuerd?\",\"Cases have been reported in how many countries?\",\"How many countries have reported cases?\",\"How long should I wash my hands?\",\"What are the common symptoms of COVID-19?\",\"What type of masks should I wear?\"]\n",
        "\n",
        "\n",
        "for j in question:\n",
        "  #j=TextBlob(j)\n",
        "  #j=str(j.correct())\n",
        "\n",
        "  qq=[]\n",
        "  qp=[]\n",
        "  que=sent_tokenize(j)\n",
        "\n",
        "  qq.append(que)\n",
        "  qp.append(j)\n",
        "  questiontags = ['WP','WDT','WP$','WRB']\n",
        "  question_POS = pos_tag(word_tokenize(j.lower()))\n",
        "\n",
        "  question_Tags=[]\n",
        "  for token in question_POS:\n",
        "      if token[1] in questiontags:\n",
        "          question_Tags.append(token)\n",
        "\n",
        "\n",
        "  if len(question_Tags)>1:\n",
        "     if ' and ' in j :\n",
        "       pos=j.lower().find('and')\n",
        "       qq=[]\n",
        "       qp=[]\n",
        "       qp.append(j[:pos])\n",
        "       qp.append(j[pos+1:])\n",
        "       qq.append(sent_tokenize(j[:pos]))\n",
        "       qq.append(sent_tokenize(j[pos+1:]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print(\"QUESTION:\",j)\n",
        "  for k in range(len(qp)):\n",
        "\n",
        "\n",
        "\n",
        "    sentences=sent_tokenize(s)\n",
        "    #q contains a list of cleaned sentence tokens of question\n",
        "    q=get_cleaned_sentences(qq[k],stopwords=True)\n",
        "    #preprocessed contains a list of cleaned sentence tokens of the reference text\n",
        "    preprocessed=get_cleaned_sentences(sentences,stopwords=True)\n",
        "\n",
        "    preprocessed.append(q[0])\n",
        "    i=len(preprocessed)-1\n",
        "\n",
        "\n",
        "    tfidf_vect=TfidfVectorizer()\n",
        "    df=create_document_term_matrix(preprocessed,tfidf_vect)\n",
        "    df_list = df.values.tolist()\n",
        "    answertype(qp[k])\n",
        "    print()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJXaVEXX+urXrYg2eWI76o",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}